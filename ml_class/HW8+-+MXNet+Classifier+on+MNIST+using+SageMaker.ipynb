{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites and Preprocessing\n",
    "\n",
    "### Permissions & environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket='beichuan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "#Bucket location to save your custom code in tar.gz format.\n",
    "custom_code_upload_location = 's3://beichuan/customcode/mxnet'\n",
    "\n",
    "#Bucket location where results of model training are saved.\n",
    "model_artifacts_location = 's3://beichuan/artifacts'\n",
    "\n",
    "#IAM execution role that gives SageMaker access to resources in your AWS account.\n",
    "#We can use the SageMaker Python SDK to get the role from our notebook environment. \n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import logging\r\n",
      "\r\n",
      "import gzip\r\n",
      "import mxnet as mx\r\n",
      "import numpy as np\r\n",
      "import os\r\n",
      "import struct\r\n",
      "\r\n",
      "\r\n",
      "def load_data(path):\r\n",
      "    with gzip.open(find_file(path, \"labels.gz\")) as flbl:\r\n",
      "        struct.unpack(\">II\", flbl.read(8))\r\n",
      "        labels = np.fromstring(flbl.read(), dtype=np.int8)\r\n",
      "    with gzip.open(find_file(path, \"images.gz\")) as fimg:\r\n",
      "        _, _, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\r\n",
      "        images = np.fromstring(fimg.read(), dtype=np.uint8).reshape(len(labels), rows, cols)\r\n",
      "        images = images.reshape(images.shape[0], 1, 28, 28).astype(np.float32) / 255\r\n",
      "    return labels, images\r\n",
      "\r\n",
      "\r\n",
      "def find_file(root_path, file_name):\r\n",
      "    for root, dirs, files in os.walk(root_path):\r\n",
      "        if file_name in files:\r\n",
      "            return os.path.join(root, file_name)\r\n",
      "\r\n",
      "\r\n",
      "def build_graph():\r\n",
      "    data = mx.sym.var('data')\r\n",
      "    data = mx.sym.flatten(data=data)\r\n",
      "    fc1 = mx.sym.FullyConnected(data=data, num_hidden=128)\r\n",
      "    act1 = mx.sym.Activation(data=fc1, act_type=\"relu\")\r\n",
      "    fc2 = mx.sym.FullyConnected(data=act1, num_hidden=64)\r\n",
      "    act2 = mx.sym.Activation(data=fc2, act_type=\"relu\")\r\n",
      "    fc3 = mx.sym.FullyConnected(data=act2, num_hidden=10)\r\n",
      "    return mx.sym.SoftmaxOutput(data=fc3, name='softmax')\r\n",
      "\r\n",
      "\r\n",
      "def train(channel_input_dirs, hyperparameters, hosts, num_cpus, num_gpus, **kwargs):\r\n",
      "    (train_labels, train_images) = load_data(os.path.join(channel_input_dirs['train']))\r\n",
      "    (test_labels, test_images) = load_data(os.path.join(channel_input_dirs['test']))\r\n",
      "    batch_size = 100\r\n",
      "    train_iter = mx.io.NDArrayIter(train_images, train_labels, batch_size, shuffle=True)\r\n",
      "    val_iter = mx.io.NDArrayIter(test_images, test_labels, batch_size)\r\n",
      "    logging.getLogger().setLevel(logging.DEBUG)\r\n",
      "    kvstore = 'local' if len(hosts) == 1 else 'dist_sync'\r\n",
      "    mlp_model = mx.mod.Module(\r\n",
      "        symbol=build_graph(),\r\n",
      "        context=get_train_context(num_cpus, num_gpus))\r\n",
      "    mlp_model.fit(train_iter,\r\n",
      "                  eval_data=val_iter,\r\n",
      "                  kvstore=kvstore,\r\n",
      "                  optimizer='sgd',\r\n",
      "                  optimizer_params={'learning_rate': float(hyperparameters.get(\"learning_rate\", 0.1))},\r\n",
      "                  eval_metric='acc',\r\n",
      "                  batch_end_callback=mx.callback.Speedometer(batch_size, 100),\r\n",
      "                  num_epoch=25)\r\n",
      "    return mlp_model\r\n",
      "\r\n",
      "\r\n",
      "def get_train_context(num_cpus, num_gpus):\r\n",
      "    if num_gpus > 0:\r\n",
      "        return mx.gpu()\r\n",
      "    return mx.cpu()\r\n"
     ]
    }
   ],
   "source": [
    "!cat mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker's MXNet estimator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "mnist_estimator = MXNet(entry_point='mnist.py',\n",
    "                        role=role,\n",
    "                        output_path=model_artifacts_location,\n",
    "                        code_location=custom_code_upload_location,\n",
    "                        train_instance_count=1, \n",
    "                        train_instance_type='ml.m4.xlarge',\n",
    "                        hyperparameters={'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-mxnet-py2-cpu-2018-03-08-05-00-57-269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................................\n",
      "\u001b[31mexecuting startup script (first run)\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:14,447 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:14,447 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:15,674 INFO - mxnet_container.train - MXNetTrainingEnvironment: {'enable_cloudwatch_metrics': False, 'available_gpus': 0, 'channels': {u'test': {u'TrainingInputMode': u'File', u'RecordWrapperType': u'None', u'S3DistributionType': u'FullyReplicated'}, u'train': {u'TrainingInputMode': u'File', u'RecordWrapperType': u'None', u'S3DistributionType': u'FullyReplicated'}}, '_ps_verbose': 0, 'resource_config': {u'current_host': u'algo-1', u'hosts': [u'algo-1']}, 'user_script_name': u'mnist.py', 'input_config_dir': '/opt/ml/input/config', 'channel_dirs': {u'test': u'/opt/ml/input/data/test', u'train': u'/opt/ml/input/data/train'}, 'code_dir': '/opt/ml/code', 'output_data_dir': '/opt/ml/output/data/', 'output_dir': '/opt/ml/output', 'model_dir': '/opt/ml/model', 'hyperparameters': {u'sagemaker_program': u'mnist.py', u'learning_rate': 0.1, u'sagemaker_submit_directory': u's3://beichuan/customcode/mxnet/sagemaker-mxnet-py2-cpu-2018-03-08-05-00-57-269/source/sourcedir.tar.gz', u'sagemaker_region': u'us-east-1', u'sagemaker_enable_cloudwatch_metrics': False, u'sagemaker_job_name': u'sagemaker-mxnet-py2-cpu-2018-03-08-05-00-57-269', u'sagemaker_container_log_level': 20}, 'hosts': [u'algo-1'], '_scheduler_ip': '10.32.0.4', '_ps_port': 8000, 'user_script_archive': u's3://beichuan/customcode/mxnet/sagemaker-mxnet-py2-cpu-2018-03-08-05-00-57-269/source/sourcedir.tar.gz', '_scheduler_host': u'algo-1', 'sagemaker_region': u'us-east-1', 'input_dir': '/opt/ml/input', 'user_requirements_file': None, 'current_host': u'algo-1', 'container_log_level': 20, 'available_cpus': 4, 'base_dir': '/opt/ml'}\u001b[0m\n",
      "\u001b[31mDownloading s3://beichuan/customcode/mxnet/sagemaker-mxnet-py2-cpu-2018-03-08-05-00-57-269/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:15,772 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:15,871 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:15,890 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.us-west-2.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:16,389 INFO - mxnet_container.train - Starting distributed training task\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:17,923 INFO - root - Epoch[0] Batch [100]#011Speed: 27521.66 samples/sec#011accuracy=0.111386\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:18,261 INFO - root - Epoch[0] Batch [200]#011Speed: 29637.70 samples/sec#011accuracy=0.114700\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:18,592 INFO - root - Epoch[0] Batch [300]#011Speed: 30165.84 samples/sec#011accuracy=0.111500\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:18,912 INFO - root - Epoch[0] Batch [400]#011Speed: 31351.41 samples/sec#011accuracy=0.109400\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:19,197 INFO - root - Epoch[0] Batch [500]#011Speed: 35055.74 samples/sec#011accuracy=0.114500\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:19,515 INFO - root - Epoch[0] Train-accuracy=0.280404\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:19,515 INFO - root - Epoch[0] Time cost=1.976\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:19,709 INFO - root - Epoch[0] Validation-accuracy=0.370800\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:20,030 INFO - root - Epoch[1] Batch [100]#011Speed: 31396.89 samples/sec#011accuracy=0.471683\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:20,358 INFO - root - Epoch[1] Batch [200]#011Speed: 30525.41 samples/sec#011accuracy=0.586700\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:20,689 INFO - root - Epoch[1] Batch [300]#011Speed: 30244.48 samples/sec#011accuracy=0.746600\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:21,013 INFO - root - Epoch[1] Batch [400]#011Speed: 30900.54 samples/sec#011accuracy=0.794400\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:21,318 INFO - root - Epoch[1] Batch [500]#011Speed: 32777.86 samples/sec#011accuracy=0.823200\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:21,706 INFO - root - Epoch[1] Train-accuracy=0.840404\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:21,706 INFO - root - Epoch[1] Time cost=1.997\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:21,883 INFO - root - Epoch[1] Validation-accuracy=0.860000\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:22,254 INFO - root - Epoch[2] Batch [100]#011Speed: 27189.50 samples/sec#011accuracy=0.865248\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:22,573 INFO - root - Epoch[2] Batch [200]#011Speed: 31398.56 samples/sec#011accuracy=0.872600\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:22,955 INFO - root - Epoch[2] Batch [300]#011Speed: 26218.45 samples/sec#011accuracy=0.887300\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:23,300 INFO - root - Epoch[2] Batch [400]#011Speed: 28963.25 samples/sec#011accuracy=0.893400\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:23,661 INFO - root - Epoch[2] Batch [500]#011Speed: 27713.97 samples/sec#011accuracy=0.909900\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:23,946 INFO - root - Epoch[2] Train-accuracy=0.913434\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:23,947 INFO - root - Epoch[2] Time cost=2.063\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:24,081 INFO - root - Epoch[2] Validation-accuracy=0.914500\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:24,324 INFO - root - Epoch[3] Batch [100]#011Speed: 41542.41 samples/sec#011accuracy=0.918614\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:24,564 INFO - root - Epoch[3] Batch [200]#011Speed: 41685.09 samples/sec#011accuracy=0.921200\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:24,836 INFO - root - Epoch[3] Batch [300]#011Speed: 36827.68 samples/sec#011accuracy=0.926900\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:25,072 INFO - root - Epoch[3] Batch [400]#011Speed: 42420.31 samples/sec#011accuracy=0.930500\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:25,311 INFO - root - Epoch[3] Batch [500]#011Speed: 41844.33 samples/sec#011accuracy=0.937900\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:25,545 INFO - root - Epoch[3] Train-accuracy=0.938687\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:25,545 INFO - root - Epoch[3] Time cost=1.464\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:25,719 INFO - root - Epoch[3] Validation-accuracy=0.936700\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:25,961 INFO - root - Epoch[4] Batch [100]#011Speed: 41873.78 samples/sec#011accuracy=0.942376\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:26,228 INFO - root - Epoch[4] Batch [200]#011Speed: 37399.39 samples/sec#011accuracy=0.944000\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:26,463 INFO - root - Epoch[4] Batch [300]#011Speed: 42664.67 samples/sec#011accuracy=0.945800\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:26,739 INFO - root - Epoch[4] Batch [400]#011Speed: 36212.86 samples/sec#011accuracy=0.947100\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:26,988 INFO - root - Epoch[4] Batch [500]#011Speed: 40252.44 samples/sec#011accuracy=0.952400\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:27,253 INFO - root - Epoch[4] Train-accuracy=0.953737\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:27,253 INFO - root - Epoch[4] Time cost=1.534\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:27,403 INFO - root - Epoch[4] Validation-accuracy=0.949300\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:27,686 INFO - root - Epoch[5] Batch [100]#011Speed: 35655.72 samples/sec#011accuracy=0.952178\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:27,994 INFO - root - Epoch[5] Batch [200]#011Speed: 32519.71 samples/sec#011accuracy=0.956400\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:28,236 INFO - root - Epoch[5] Batch [300]#011Speed: 41298.94 samples/sec#011accuracy=0.957100\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:28,481 INFO - root - Epoch[5] Batch [400]#011Speed: 40952.08 samples/sec#011accuracy=0.956300\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:28,793 INFO - root - Epoch[5] Batch [500]#011Speed: 32011.06 samples/sec#011accuracy=0.962600\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:29,031 INFO - root - Epoch[5] Train-accuracy=0.962020\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:29,032 INFO - root - Epoch[5] Time cost=1.629\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:29,165 INFO - root - Epoch[5] Validation-accuracy=0.955400\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:29,471 INFO - root - Epoch[6] Batch [100]#011Speed: 32982.62 samples/sec#011accuracy=0.961980\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:29,788 INFO - root - Epoch[6] Batch [200]#011Speed: 31552.82 samples/sec#011accuracy=0.963100\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:30,027 INFO - root - Epoch[6] Batch [300]#011Speed: 41862.75 samples/sec#011accuracy=0.963300\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:30,262 INFO - root - Epoch[6] Batch [400]#011Speed: 42520.81 samples/sec#011accuracy=0.963500\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:30,499 INFO - root - Epoch[6] Batch [500]#011Speed: 42242.20 samples/sec#011accuracy=0.968200\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:30,768 INFO - root - Epoch[6] Train-accuracy=0.966768\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:30,769 INFO - root - Epoch[6] Time cost=1.603\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:30,893 INFO - root - Epoch[6] Validation-accuracy=0.959800\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:31,133 INFO - root - Epoch[7] Batch [100]#011Speed: 41990.70 samples/sec#011accuracy=0.966337\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:31,373 INFO - root - Epoch[7] Batch [200]#011Speed: 41711.33 samples/sec#011accuracy=0.967100\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:31,646 INFO - root - Epoch[7] Batch [300]#011Speed: 36718.38 samples/sec#011accuracy=0.968200\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:31,956 INFO - root - Epoch[7] Batch [400]#011Speed: 32227.61 samples/sec#011accuracy=0.968400\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:32,202 INFO - root - Epoch[7] Batch [500]#011Speed: 40780.19 samples/sec#011accuracy=0.973000\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:32,472 INFO - root - Epoch[7] Train-accuracy=0.971111\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:32,472 INFO - root - Epoch[7] Time cost=1.580\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:32,648 INFO - root - Epoch[7] Validation-accuracy=0.964500\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:32,937 INFO - root - Epoch[8] Batch [100]#011Speed: 34917.79 samples/sec#011accuracy=0.969901\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:33,216 INFO - root - Epoch[8] Batch [200]#011Speed: 35873.93 samples/sec#011accuracy=0.971800\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:33,526 INFO - root - Epoch[8] Batch [300]#011Speed: 32239.55 samples/sec#011accuracy=0.971800\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:33,880 INFO - root - Epoch[8] Batch [400]#011Speed: 28297.25 samples/sec#011accuracy=0.971800\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:34,183 INFO - root - Epoch[8] Batch [500]#011Speed: 33018.55 samples/sec#011accuracy=0.976700\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:34,432 INFO - root - Epoch[8] Train-accuracy=0.975657\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:34,432 INFO - root - Epoch[8] Time cost=1.783\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:34,578 INFO - root - Epoch[8] Validation-accuracy=0.967100\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:34,901 INFO - root - Epoch[9] Batch [100]#011Speed: 31113.68 samples/sec#011accuracy=0.974356\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:35,155 INFO - root - Epoch[9] Batch [200]#011Speed: 39439.94 samples/sec#011accuracy=0.976100\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:35,427 INFO - root - Epoch[9] Batch [300]#011Speed: 36793.27 samples/sec#011accuracy=0.976500\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:35,774 INFO - root - Epoch[9] Batch [400]#011Speed: 28850.63 samples/sec#011accuracy=0.974100\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:36,067 INFO - root - Epoch[9] Batch [500]#011Speed: 34195.17 samples/sec#011accuracy=0.979400\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:36,308 INFO - root - Epoch[9] Train-accuracy=0.978687\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:36,308 INFO - root - Epoch[9] Time cost=1.730\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:36,432 INFO - root - Epoch[9] Validation-accuracy=0.969100\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:36,766 INFO - root - Epoch[10] Batch [100]#011Speed: 30130.48 samples/sec#011accuracy=0.977030\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:37,025 INFO - root - Epoch[10] Batch [200]#011Speed: 38573.85 samples/sec#011accuracy=0.978900\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:37,329 INFO - root - Epoch[10] Batch [300]#011Speed: 33007.87 samples/sec#011accuracy=0.980300\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:37,576 INFO - root - Epoch[10] Batch [400]#011Speed: 40488.30 samples/sec#011accuracy=0.977600\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:37,865 INFO - root - Epoch[10] Batch [500]#011Speed: 34646.80 samples/sec#011accuracy=0.981300\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:38,103 INFO - root - Epoch[10] Train-accuracy=0.981111\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:38,103 INFO - root - Epoch[10] Time cost=1.671\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:38,229 INFO - root - Epoch[10] Validation-accuracy=0.970400\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:38,471 INFO - root - Epoch[11] Batch [100]#011Speed: 41807.58 samples/sec#011accuracy=0.979703\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:38,746 INFO - root - Epoch[11] Batch [200]#011Speed: 36338.51 samples/sec#011accuracy=0.981800\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:38,983 INFO - root - Epoch[11] Batch [300]#011Speed: 42245.65 samples/sec#011accuracy=0.982500\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:39,220 INFO - root - Epoch[11] Batch [400]#011Speed: 42357.25 samples/sec#011accuracy=0.980300\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:39,457 INFO - root - Epoch[11] Batch [500]#011Speed: 42200.84 samples/sec#011accuracy=0.984300\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:39,725 INFO - root - Epoch[11] Train-accuracy=0.983030\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:39,725 INFO - root - Epoch[11] Time cost=1.496\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:39,872 INFO - root - Epoch[11] Validation-accuracy=0.971500\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:40,111 INFO - root - Epoch[12] Batch [100]#011Speed: 42327.67 samples/sec#011accuracy=0.981683\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:40,350 INFO - root - Epoch[12] Batch [200]#011Speed: 41782.47 samples/sec#011accuracy=0.984200\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:40,589 INFO - root - Epoch[12] Batch [300]#011Speed: 41984.14 samples/sec#011accuracy=0.984300\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:40,857 INFO - root - Epoch[12] Batch [400]#011Speed: 37227.31 samples/sec#011accuracy=0.982100\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:41,093 INFO - root - Epoch[12] Batch [500]#011Speed: 42496.04 samples/sec#011accuracy=0.987300\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:41,328 INFO - root - Epoch[12] Train-accuracy=0.985455\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:41,328 INFO - root - Epoch[12] Time cost=1.456\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:41,452 INFO - root - Epoch[12] Validation-accuracy=0.973100\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:41,724 INFO - root - Epoch[13] Batch [100]#011Speed: 37071.48 samples/sec#011accuracy=0.984158\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:41,984 INFO - root - Epoch[13] Batch [200]#011Speed: 38432.43 samples/sec#011accuracy=0.986400\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:42,219 INFO - root - Epoch[13] Batch [300]#011Speed: 42621.40 samples/sec#011accuracy=0.987000\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:42,456 INFO - root - Epoch[13] Batch [400]#011Speed: 42251.31 samples/sec#011accuracy=0.983900\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:42,727 INFO - root - Epoch[13] Batch [500]#011Speed: 37003.74 samples/sec#011accuracy=0.988100\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:42,960 INFO - root - Epoch[13] Train-accuracy=0.987677\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:42,960 INFO - root - Epoch[13] Time cost=1.508\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:43,084 INFO - root - Epoch[13] Validation-accuracy=0.973600\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:43,326 INFO - root - Epoch[14] Batch [100]#011Speed: 41661.45 samples/sec#011accuracy=0.986040\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:43,565 INFO - root - Epoch[14] Batch [200]#011Speed: 41815.46 samples/sec#011accuracy=0.988700\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:43,857 INFO - root - Epoch[14] Batch [300]#011Speed: 34302.36 samples/sec#011accuracy=0.989000\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:44,094 INFO - root - Epoch[14] Batch [400]#011Speed: 42278.48 samples/sec#011accuracy=0.986500\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:44,332 INFO - root - Epoch[14] Batch [500]#011Speed: 42018.21 samples/sec#011accuracy=0.989800\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:44,567 INFO - root - Epoch[14] Train-accuracy=0.988687\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:44,567 INFO - root - Epoch[14] Time cost=1.483\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:44,726 INFO - root - Epoch[14] Validation-accuracy=0.974500\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:44,965 INFO - root - Epoch[15] Batch [100]#011Speed: 42384.17 samples/sec#011accuracy=0.989109\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:45,200 INFO - root - Epoch[15] Batch [200]#011Speed: 42638.30 samples/sec#011accuracy=0.989900\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:45,436 INFO - root - Epoch[15] Batch [300]#011Speed: 42276.86 samples/sec#011accuracy=0.990700\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:45,707 INFO - root - Epoch[15] Batch [400]#011Speed: 37006.49 samples/sec#011accuracy=0.987700\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:45,967 INFO - root - Epoch[15] Batch [500]#011Speed: 38382.00 samples/sec#011accuracy=0.992000\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:46,200 INFO - root - Epoch[15] Train-accuracy=0.990000\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:46,200 INFO - root - Epoch[15] Time cost=1.474\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:46,325 INFO - root - Epoch[15] Validation-accuracy=0.974800\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:46,567 INFO - root - Epoch[16] Batch [100]#011Speed: 41783.30 samples/sec#011accuracy=0.990495\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:46,838 INFO - root - Epoch[16] Batch [200]#011Speed: 36962.82 samples/sec#011accuracy=0.991000\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:47,074 INFO - root - Epoch[16] Batch [300]#011Speed: 42415.29 samples/sec#011accuracy=0.991500\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:47,308 INFO - root - Epoch[16] Batch [400]#011Speed: 42616.29 samples/sec#011accuracy=0.990000\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:47,545 INFO - root - Epoch[16] Batch [500]#011Speed: 42384.56 samples/sec#011accuracy=0.992900\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:47,815 INFO - root - Epoch[16] Train-accuracy=0.991717\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:47,815 INFO - root - Epoch[16] Time cost=1.490\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:47,979 INFO - root - Epoch[16] Validation-accuracy=0.975100\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:48,292 INFO - root - Epoch[17] Batch [100]#011Speed: 32195.65 samples/sec#011accuracy=0.991782\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:48,600 INFO - root - Epoch[17] Batch [200]#011Speed: 32414.93 samples/sec#011accuracy=0.991800\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:48,925 INFO - root - Epoch[17] Batch [300]#011Speed: 30813.42 samples/sec#011accuracy=0.993000\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:49,178 INFO - root - Epoch[17] Batch [400]#011Speed: 39561.81 samples/sec#011accuracy=0.991200\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:49,461 INFO - root - Epoch[17] Batch [500]#011Speed: 35427.20 samples/sec#011accuracy=0.994300\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:49,727 INFO - root - Epoch[17] Train-accuracy=0.993131\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:49,727 INFO - root - Epoch[17] Time cost=1.748\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:49,852 INFO - root - Epoch[17] Validation-accuracy=0.975200\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:50,170 INFO - root - Epoch[18] Batch [100]#011Speed: 31707.99 samples/sec#011accuracy=0.993366\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:50,415 INFO - root - Epoch[18] Batch [200]#011Speed: 40944.65 samples/sec#011accuracy=0.993700\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:50,705 INFO - root - Epoch[18] Batch [300]#011Speed: 34513.81 samples/sec#011accuracy=0.994300\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:51,016 INFO - root - Epoch[18] Batch [400]#011Speed: 32148.34 samples/sec#011accuracy=0.992400\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:51,276 INFO - root - Epoch[18] Batch [500]#011Speed: 38514.55 samples/sec#011accuracy=0.995700\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:51,555 INFO - root - Epoch[18] Train-accuracy=0.994141\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:51,555 INFO - root - Epoch[18] Time cost=1.703\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:51,722 INFO - root - Epoch[18] Validation-accuracy=0.975600\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:51,984 INFO - root - Epoch[19] Batch [100]#011Speed: 38517.10 samples/sec#011accuracy=0.994653\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:52,241 INFO - root - Epoch[19] Batch [200]#011Speed: 38941.87 samples/sec#011accuracy=0.994500\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:52,555 INFO - root - Epoch[19] Batch [300]#011Speed: 31805.00 samples/sec#011accuracy=0.994600\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:52,892 INFO - root - Epoch[19] Batch [400]#011Speed: 29732.71 samples/sec#011accuracy=0.993600\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:53,168 INFO - root - Epoch[19] Batch [500]#011Speed: 36301.87 samples/sec#011accuracy=0.996300\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:53,406 INFO - root - Epoch[19] Train-accuracy=0.995051\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:53,406 INFO - root - Epoch[19] Time cost=1.684\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:53,564 INFO - root - Epoch[19] Validation-accuracy=0.976000\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:53,865 INFO - root - Epoch[20] Batch [100]#011Speed: 33455.67 samples/sec#011accuracy=0.995545\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:54,177 INFO - root - Epoch[20] Batch [200]#011Speed: 32158.15 samples/sec#011accuracy=0.995600\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:54,477 INFO - root - Epoch[20] Batch [300]#011Speed: 33364.57 samples/sec#011accuracy=0.995700\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:54,833 INFO - root - Epoch[20] Batch [400]#011Speed: 28040.65 samples/sec#011accuracy=0.994600\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:55,134 INFO - root - Epoch[20] Batch [500]#011Speed: 33313.91 samples/sec#011accuracy=0.997000\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:55,436 INFO - root - Epoch[20] Train-accuracy=0.995758\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:55,436 INFO - root - Epoch[20] Time cost=1.873\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:55,579 INFO - root - Epoch[20] Validation-accuracy=0.976400\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:55,944 INFO - root - Epoch[21] Batch [100]#011Speed: 27607.07 samples/sec#011accuracy=0.996436\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:56,300 INFO - root - Epoch[21] Batch [200]#011Speed: 28103.24 samples/sec#011accuracy=0.996600\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:56,577 INFO - root - Epoch[21] Batch [300]#011Speed: 36108.64 samples/sec#011accuracy=0.996900\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:56,947 INFO - root - Epoch[21] Batch [400]#011Speed: 27058.19 samples/sec#011accuracy=0.995400\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:57,260 INFO - root - Epoch[21] Batch [500]#011Speed: 32000.32 samples/sec#011accuracy=0.997600\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:57,552 INFO - root - Epoch[21] Train-accuracy=0.996667\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:57,553 INFO - root - Epoch[21] Time cost=1.973\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:57,725 INFO - root - Epoch[21] Validation-accuracy=0.976900\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:58,078 INFO - root - Epoch[22] Batch [100]#011Speed: 28569.06 samples/sec#011accuracy=0.997327\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:58,413 INFO - root - Epoch[22] Batch [200]#011Speed: 29845.03 samples/sec#011accuracy=0.996900\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:58,745 INFO - root - Epoch[22] Batch [300]#011Speed: 30163.63 samples/sec#011accuracy=0.996900\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:59,026 INFO - root - Epoch[22] Batch [400]#011Speed: 35658.75 samples/sec#011accuracy=0.996400\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:59,314 INFO - root - Epoch[22] Batch [500]#011Speed: 34683.30 samples/sec#011accuracy=0.997900\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:59,594 INFO - root - Epoch[22] Train-accuracy=0.997273\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:59,594 INFO - root - Epoch[22] Time cost=1.869\u001b[0m\n",
      "\u001b[31m2018-03-08 05:06:59,765 INFO - root - Epoch[22] Validation-accuracy=0.976200\u001b[0m\n",
      "\u001b[31m2018-03-08 05:07:00,091 INFO - root - Epoch[23] Batch [100]#011Speed: 30966.18 samples/sec#011accuracy=0.997822\u001b[0m\n",
      "\u001b[31m2018-03-08 05:07:00,395 INFO - root - Epoch[23] Batch [200]#011Speed: 32909.36 samples/sec#011accuracy=0.997500\u001b[0m\n",
      "\u001b[31m2018-03-08 05:07:00,726 INFO - root - Epoch[23] Batch [300]#011Speed: 30205.36 samples/sec#011accuracy=0.997600\u001b[0m\n",
      "\u001b[31m2018-03-08 05:07:01,042 INFO - root - Epoch[23] Batch [400]#011Speed: 31644.26 samples/sec#011accuracy=0.997300\u001b[0m\n",
      "\u001b[31m2018-03-08 05:07:01,350 INFO - root - Epoch[23] Batch [500]#011Speed: 32534.82 samples/sec#011accuracy=0.998200\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m2018-03-08 05:07:01,763 INFO - root - Epoch[23] Train-accuracy=0.997475\u001b[0m\n",
      "\u001b[31m2018-03-08 05:07:01,763 INFO - root - Epoch[23] Time cost=1.997\u001b[0m\n",
      "\u001b[31m2018-03-08 05:07:01,903 INFO - root - Epoch[23] Validation-accuracy=0.976000\u001b[0m\n",
      "\u001b[31m2018-03-08 05:07:02,314 INFO - root - Epoch[24] Batch [100]#011Speed: 24464.11 samples/sec#011accuracy=0.997921\u001b[0m\n",
      "\u001b[31m2018-03-08 05:07:02,696 INFO - root - Epoch[24] Batch [200]#011Speed: 26227.51 samples/sec#011accuracy=0.998100\u001b[0m\n",
      "\u001b[31m2018-03-08 05:07:03,025 INFO - root - Epoch[24] Batch [300]#011Speed: 30355.37 samples/sec#011accuracy=0.998500\u001b[0m\n",
      "\u001b[31m2018-03-08 05:07:03,388 INFO - root - Epoch[24] Batch [400]#011Speed: 27597.63 samples/sec#011accuracy=0.997600\u001b[0m\n",
      "\u001b[31m2018-03-08 05:07:03,719 INFO - root - Epoch[24] Batch [500]#011Speed: 30209.73 samples/sec#011accuracy=0.998600\u001b[0m\n",
      "\u001b[31m2018-03-08 05:07:04,059 INFO - root - Epoch[24] Train-accuracy=0.998081\u001b[0m\n",
      "\u001b[31m2018-03-08 05:07:04,059 INFO - root - Epoch[24] Time cost=2.156\u001b[0m\n",
      "\u001b[31m2018-03-08 05:07:04,244 INFO - root - Epoch[24] Validation-accuracy=0.976200\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error training sagemaker-mxnet-py2-cpu-2018-03-08-05-00-57-269: Failed Reason: ClientError: Model artifact upload failed:Error 7: Supplied bucket is not in the region associated with the training job (us-east-1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJOB_NAME_PARAM_NAME\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSAGEMAKER_REGION_PARAM_NAME\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregion_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFramework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Asynchronous fit not available'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Completed'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FailureReason'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'(No reason provided)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error training {}: {} Reason: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait_for_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error training sagemaker-mxnet-py2-cpu-2018-03-08-05-00-57-269: Failed Reason: ClientError: Model artifact upload failed:Error 7: Supplied bucket is not in the region associated with the training job (us-east-1)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "train_data_location = 's3://sagemaker-sample-data-{}/mxnet/mnist/train'.format(region)\n",
    "test_data_location = 's3://sagemaker-sample-data-{}/mxnet/mnist/test'.format(region)\n",
    "\n",
    "mnist_estimator.fit({'train': train_data_location, 'test': test_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an inference Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictor = mnist_estimator.deploy(initial_instance_count=1,\n",
    "                                   instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making an inference request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open(\"input.html\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = predictor.predict(data)\n",
    "print('Raw prediction result:')\n",
    "print(response)\n",
    "\n",
    "labeled_predictions = list(zip(range(10), response[0]))\n",
    "print('Labeled predictions: ')\n",
    "print(labeled_predictions)\n",
    "\n",
    "labeled_predictions.sort(key=lambda label_and_prob: 1.0 - label_and_prob[1])\n",
    "print('Most likely answer: {}'.format(labeled_predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: \n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
